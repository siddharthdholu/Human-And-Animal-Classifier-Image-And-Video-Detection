{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6097ea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def extract_features(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "        image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "    return image_features.cpu().numpy().flatten()\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "data_root = \"data\"\n",
    "classes = os.listdir(data_root)\n",
    "\n",
    "for label_idx, class_name in enumerate(classes):\n",
    "    class_dir = os.path.join(data_root, class_name)\n",
    "    for img_name in os.listdir(class_dir):\n",
    "        img_path = os.path.join(class_dir, img_name)\n",
    "        try:\n",
    "            embedding = extract_features(img_path)\n",
    "            features.append(embedding)\n",
    "            labels.append(label_idx)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "033d6949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Accuracy: 97.72%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(max_iter=5000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Classifier Accuracy: {acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed35ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['human_animal_classifier.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump(clf, 'human_animal_classifier.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c4708b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cat', 'Cow', 'Deer', 'Dog', 'Goat', 'Human', 'Sheep']\n"
     ]
    }
   ],
   "source": [
    "data_root = \"data\"\n",
    "classes = os.listdir(data_root)\n",
    "label_classes = []\n",
    "\n",
    "for label_idx, class_name in enumerate(classes):\n",
    "    label_classes.append(class_name)\n",
    "\n",
    "print(label_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b98e59f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from joblib import load\n",
    "\n",
    "clf = load('model/human_animal_classifier.joblib')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "class_labels = label_classes\n",
    "\n",
    "def extract_features_pil(image):\n",
    "    \"\"\"Extract CLIP features from a PIL image.\"\"\"\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "        features = features / features.norm(p=2, dim=-1, keepdim=True)\n",
    "    return features.cpu().numpy().flatten()\n",
    "\n",
    "def predict_class(features):\n",
    "    \"\"\"Predict class label and confidence using trained classifier.\"\"\"\n",
    "    prob = clf.predict_proba([features])[0]\n",
    "    predicted_idx = prob.argmax()\n",
    "    confidence = prob[predicted_idx]\n",
    "    label = class_labels[predicted_idx]\n",
    "    return label, confidence\n",
    "\n",
    "def detect_from_path(path, is_image=True):\n",
    "    if is_image:\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        features = extract_features_pil(image)\n",
    "        label, confidence = predict_class(features)\n",
    "        print(f\"[IMAGE] Detected: {label} (Confidence: {confidence*100:.2f}%)\")\n",
    "        return label, confidence\n",
    "\n",
    "    else:\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            features = extract_features_pil(image)\n",
    "            label, confidence = predict_class(features)\n",
    "\n",
    "            cv2.putText(frame, f\"{label} ({confidence*100:.1f}%)\", (10,30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "            cv2.imshow('Detection', frame)\n",
    "\n",
    "            if confidence > 0.80:\n",
    "                print(f\"[VIDEO] ALERT: Detected {label} (Confidence: {confidence*100:.2f}%)\")\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        return None, None\n",
    "\n",
    "# === Usage ===\n",
    "# For single image:\n",
    "# detect_from_path(\"test_images/sample.jpg\", is_image=True)\n",
    "\n",
    "# For video:\n",
    "# detect_from_path(\"test_videos/sample_video.mp4\", is_image=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2031719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VIDEO] ALERT: Detected Dog (Confidence: 80.50%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 82.74%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 85.58%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 85.25%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 81.58%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 88.46%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 88.74%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 90.55%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 87.03%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 89.02%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 91.11%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 88.66%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 88.25%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 83.07%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 89.22%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 84.67%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 80.17%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 83.53%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 86.48%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 88.50%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 90.90%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 90.18%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 83.75%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 85.24%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 85.47%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 85.05%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 80.84%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 86.53%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 87.18%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 87.14%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 86.74%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 87.62%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 83.74%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 83.33%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 90.15%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 88.09%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 92.54%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 89.85%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 89.27%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 86.10%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 89.94%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 90.72%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 84.88%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 84.46%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 80.14%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 86.23%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 84.96%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 87.58%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 90.96%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 88.15%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 86.78%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 86.14%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 86.16%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 88.35%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 82.34%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 80.90%)\n",
      "[VIDEO] ALERT: Detected Cat (Confidence: 81.80%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 80.72%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 81.89%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 80.11%)\n",
      "[VIDEO] ALERT: Detected Dog (Confidence: 80.34%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_from_path(\"data_samples/dog_426_240_30fps.mp4\", is_image=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf355c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IMAGE] Detected: Deer (Confidence: 52.24%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Deer', np.float64(0.5223655692271296))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_from_path(\"data_samples/human.jpg\", is_image=True) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
